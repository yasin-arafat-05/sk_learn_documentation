
# 1.1.2. Ridge regression and classification¶

### Ridge Regression or Ridge Regularization

[Ridge_Regression](https://www.youtube.com/watch?v=Yj7sIK0VMg0&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=19&ab_channel=5MinutesEngineering)

<br>

Regularization: 
উপরের equation এ, x3 এর regression coefficient হচ্ছে 39 । y এর মান নির্ণয় করার জন্য এর (৩৯) importance অনেক বেশি । আর আমাদের কাছে অনেক অনেক বেশি dataset থাকবে সব গুলোর সাথে 39 গুন করতে হবে বা regression coefficient এর ভ্যালু আরো অনেক বেশি বড় হতে পারে । তাই এই গুন computationally expensive হয়ে যায় এবং model অনেক বেশি complex হয়ে যায় । এই complexity  কে reduce করার জন্য এবং regression coefficient এর  value shrink করার জন্য আমরা regularization ব্যবহার করি । 

Regularization দুই ধরনেরঃ
- Ridge 
- Lasso

Ridge Regression or Regularization: 
Ridge Regression = Loss + penalty .
Loss:  Difference between actual value and predicted value (model -> straight line) । 
Penalty: Penalty দিলে loss কম হবে । আর regression coefficient এর value reduce হবে । 

Where, alpha is a constant এবং w হচ্ছে regression coefficient .  graph গুলোতে alpha এর মান যত বাড়তেছে regression coefficient গুলোর মান কমতে থাকে । 

`Penalty যত বাড়বে loss তত কমবে তাই alpha এর মান বাড়লে loss কম হবে । `

<br>

`In machine learning, a hyperparameter is a parameter, such as the learning rate or choice of optimizer, which specifies details of the learning process, hence the name hyperparameter.`


![Alt text](/Supervised_Learning/1.1LinearModel/images/image.png)


Alpha এর মান বাড়ানোর সাথে সাথে regression coefficient এর value 39 থেকে 5 এ নেমে গিয়েছে । 




1. **Ridge Regression**: Ridge regression is a variation of linear regression that addresses some of the problems of Ordinary Least Squares (OLS) by imposing a penalty on the size of the coefficients. This penalty term is proportional to the square of the magnitude of coefficients. The ridge coefficients minimize a penalized residual sum of squares.

2. **Mathematical Formulation**: The equation provided in the documentation represents the objective function of ridge regression. Here, the first term represents the ordinary least squares loss, and the second term represents the penalty term (regularization term) which is controlled by the hyperparameter $\(\alpha\)$. The larger the value of $\(\alpha\)$, the greater the penalty on the size of coefficients.

3. **Complexity Parameter**: The hyperparameter $\(\alpha\)$ controls the amount of shrinkage applied to the coefficients. Higher values of $\(\alpha\)$ lead to greater shrinkage, making the coefficients more robust to collinearity (when features are correlated).

4. **Usage**: Similar to LinearRegression, Ridge regression in scikit-learn is used by creating an instance of the Ridge class and then calling its `fit()` method with arrays X and y. After fitting, you can access the coefficients of the linear model using the `coef_` attribute of the Ridge instance. Additionally, you can access the intercept term using the `intercept_` attribute.

5. **Example**: Here's an example provided in the documentation:

   ```python
   from sklearn import linear_model
   reg = linear_model.Ridge(alpha=.5)
   reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
   print(reg.coef_)  # Output: array([0.34545455, 0.34545455])
   print(reg.intercept_)  # Output: 0.13636...
   ```
   This example demonstrates fitting a Ridge regression model with three data points `[[0, 0], [0, 0], [1, 1]]` and corresponding target values `[0, .1, 1]`. After fitting, it prints out the coefficients of the linear model and the intercept term.

6. **Solver Selection**: The Ridge class allows users to specify the solver algorithm through the `solver` parameter. If set to "auto", scikit-learn will automatically choose between different solvers based on certain conditions, as described in the table provided in the documentation.


**Solver algorithm:** <br>
In the context of the Ridge regression algorithm in scikit-learn, the "solver" algorithm refers to the method used to solve the **optimization problem** involved in estimating the coefficients of the Ridge regression model. Ridge regression aims to find the coefficients that minimize the sum of squared differences between the observed target values and the values predicted by the linear model, while also penalizing large coefficients to prevent overfitting.

Scikit-learn provides several solver algorithms for Ridge regression, each with its own advantages and computational characteristics. These solvers include:

1. **Auto**: When "auto" is specified, scikit-learn automatically chooses the solver based on certain conditions and properties of the problem. It selects between different solvers such as "svd", "cholesky", "lsqr", "sparse_cg", and "sag" depending on the data and problem size.

2. **svd**: This solver uses Singular Value Decomposition (SVD) to compute the Ridge regression coefficients. It is generally efficient for small to medium-sized datasets.

3. **cholesky**: This solver uses the Cholesky decomposition to compute the Ridge regression coefficients. It is efficient for small to medium-sized datasets and guarantees positive-definite solutions.

4. **lsqr**: This solver uses the Least Squares QR factorization approach to compute the Ridge regression coefficients. It is suitable for large-scale problems where the number of samples is greater than the number of features.

5. **sparse_cg**: This solver uses the Conjugate Gradient (CG) method to solve the sparse linear systems involved in Ridge regression. It is efficient for large-scale sparse datasets.

6. **sag**: This solver uses Stochastic Average Gradient (SAG) optimization to solve the Ridge regression problem. It is particularly suitable for large datasets and provides faster convergence rates compared to other solvers.

Each solver has its own strengths and weaknesses, and the choice of solver can impact the computational efficiency and numerical stability of the Ridge regression estimation process. The "auto" option allows scikit-learn to automatically select the most appropriate solver based on the characteristics of the problem at hand.

<br>

### What is optimization problem?
An optimization problem involves finding the best solution from all feasible solutions. In the context of machine learning and statistical modeling, optimization problems often arise when we need to minimize or maximize an objective function, subject to certain constraints.

<br>

### Code implementation:
To implement Ridge regression with different solver algorithms in your code using scikit-learn, you can follow these steps:

1. Import the necessary modules from scikit-learn.
2. Create an instance of the Ridge regression model (`Ridge`).
3. Specify the solver algorithm through the `solver` parameter.
4. Fit the model to your training data.
5. Use the trained model to make predictions on new data if needed.

Here's an example demonstrating how to implement Ridge regression with different solver algorithms:

```python
from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Step 1: Generate some synthetic data for demonstration
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Create an instance of the Ridge regression model
ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength with the alpha parameter

# Step 3: Specify the solver algorithm (optional - "auto" is default)
solver = "auto"  # You can also specify other solvers like "svd", "cholesky", "lsqr", "sparse_cg", or "sag"
ridge.set_params(solver=solver)

# Step 4: Fit the model to your training data
ridge.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = ridge.predict(X_test)

# Optional: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
```

In this example:
- We generate synthetic data using `make_regression` for demonstration purposes.
- We split the data into training and testing sets using `train_test_split`.
- We create an instance of the Ridge regression model with `Ridge(alpha=1.0)`, where `alpha` is the regularization strength.
- We specify the solver algorithm (optional) through the `solver` parameter. Here, it's set to `"auto"`, but you can choose other options like `"svd"`, `"cholesky"`, `"lsqr"`, `"sparse_cg"`, or `"sag"`.
- We fit the Ridge regression model to the training data using `ridge.fit`.
- We make predictions on the test data using `ridge.predict`.
- We optionally evaluate the model's performance using a metric like Mean Squared Error.

You can adjust the parameters and solver algorithm according to your specific problem and dataset.


<br> <br> <br>

# 1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation

The `RidgeCV` class in scikit-learn implements ridge regression with built-in cross-validation to automatically select the optimal regularization parameter (alpha). It works similarly to `GridSearchCV`, but it defaults to Leave-One-Out Cross-Validation (LOOCV), where each observation serves as a validation set once, and the rest form the training set. This allows for an exhaustive search over the specified range of alpha values.


```python
import numpy as np
from sklearn import linear_model
reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
reg.alpha_
```

1. **Code Explanation**:
   - `import numpy as np`: This imports the NumPy library and renames it as `np` for convenience.
   - `from sklearn import linear_model`: This imports the `linear_model` module from scikit-learn, which contains functions and classes related to linear models.
   - `reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))`: This line creates an instance of the `RidgeCV` class, which implements Ridge regression with built-in cross-validation of the alpha parameter. It sets the values of the regularization parameter (`alpha`) to be tested during cross-validation using `np.logspace(-6, 6, 13)`, which generates 13 values of `alpha` logarithmically spaced between \(10^{-6}\) and \(10^6\).
   - `reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])`: This line fits the RidgeCV model to the provided data. It takes a list of feature vectors `[[0, 0], [0, 0], [1, 1]]` and a list of target values `[0, .1, 1]`.
   - `reg.alpha_`: After fitting the model, this attribute of the `RidgeCV` object (`reg`) gives the selected value of the regularization parameter (`alpha`), which minimizes the cross-validated mean squared error.

2. **Cross-Validation**:
   - Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets (folds). The model is trained on a subset of the data and tested on the remaining subset, and this process is repeated for each fold. Cross-validation helps to evaluate how well a model generalizes to unseen data and provides an estimate of the model's performance.
  
3. **GridSearchCV**:
   - `GridSearchCV` is a method in scikit-learn used for hyperparameter tuning, which involves searching for the best combination of hyperparameter values for a given model. It works by exhaustively searching through a specified grid of hyperparameter values and evaluating the model's performance using cross-validation.
  
4. **Leave-One-Out Cross-Validation (LOOCV)**:
   - LOOCV is a specific type of cross-validation where each data point is used as the test set once, with the rest of the data used for training. It is a special case of k-fold cross-validation where k is equal to the number of samples in the dataset. LOOCV is computationally expensive but provides an unbiased estimate of model performance, especially for small datasets.

In summary, the provided code demonstrates the use of `RidgeCV` for Ridge regression with built-in cross-validation of the regularization parameter (`alpha`). It automatically selects the best `alpha` value using either Leave-One-Out Cross-Validation (default) or k-fold cross-validation. The `alpha` value selected by `RidgeCV` minimizes the mean squared error on the validation sets during cross-validation.

**Hyperparameter Tuning:** <br>
Hyperparameter tuning, also known as hyperparameter optimization, refers to the process of finding the best set of hyperparameters for a machine learning algorithm. Hyperparameters are configuration settings that are external to the model and cannot be directly learned from the data during training. These settings affect the behavior and performance of the model, such as the complexity of the model, the learning rate, the regularization strength, etc.