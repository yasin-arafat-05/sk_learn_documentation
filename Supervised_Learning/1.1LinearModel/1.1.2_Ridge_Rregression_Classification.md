
# 1.1.2. Ridge regression and classification¶

### Ridge Regression or Ridge Regularization

[Ridge_Regression](https://www.youtube.com/watch?v=Yj7sIK0VMg0&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=19&ab_channel=5MinutesEngineering)

<br>

Regularization: 
উপরের equation এ, x3 এর regression coefficient হচ্ছে 39 । y এর মান নির্ণয় করার জন্য এর (৩৯) importance অনেক বেশি । আর আমাদের কাছে অনেক অনেক বেশি dataset থাকবে সব গুলোর সাথে 39 গুন করতে হবে বা regression coefficient এর ভ্যালু আরো অনেক বেশি বড় হতে পারে । তাই এই গুন computationally expensive হয়ে যায় এবং model অনেক বেশি complex হয়ে যায় । এই complexity  কে reduce করার জন্য এবং regression coefficient এর  value shrink করার জন্য আমরা regularization ব্যবহার করি । 

Regularization দুই ধরনেরঃ
- Ridge 
- Lasso

Ridge Regression or Regularization: 
Ridge Regression = Loss + penalty .
Loss:  Difference between actual value and predicted value (model -> straight line) । 
Penalty: Penalty দিলে loss কম হবে । আর regression coefficient এর value reduce হবে । 

Where, alpha is a constant এবং w হচ্ছে regression coefficient .  graph গুলোতে alpha এর মান যত বাড়তেছে regression coefficient গুলোর মান কমতে থাকে । 

`Penalty যত বাডবে loss তত কমবে তাই alpha এর মান ব`

<br>

`In machine learning, a hyperparameter is a parameter, such as the learning rate or choice of optimizer, which specifies details of the learning process, hence the name hyperparameter.`




Alpha এর মান বাড়ানোর সাথে সাথে regression coefficient এর value 39 থেকে 5 এ নেমে গিয়েছে । 




1. **Ridge Regression**: Ridge regression is a variation of linear regression that addresses some of the problems of Ordinary Least Squares (OLS) by imposing a penalty on the size of the coefficients. This penalty term is proportional to the square of the magnitude of coefficients. The ridge coefficients minimize a penalized residual sum of squares.

2. **Mathematical Formulation**: The equation provided in the documentation represents the objective function of ridge regression. Here, the first term represents the ordinary least squares loss, and the second term represents the penalty term (regularization term) which is controlled by the hyperparameter $\(\alpha\)$. The larger the value of $\(\alpha\)$, the greater the penalty on the size of coefficients.

3. **Complexity Parameter**: The hyperparameter $\(\alpha\)$ controls the amount of shrinkage applied to the coefficients. Higher values of $\(\alpha\)$ lead to greater shrinkage, making the coefficients more robust to collinearity (when features are correlated).

4. **Usage**: Similar to LinearRegression, Ridge regression in scikit-learn is used by creating an instance of the Ridge class and then calling its `fit()` method with arrays X and y. After fitting, you can access the coefficients of the linear model using the `coef_` attribute of the Ridge instance. Additionally, you can access the intercept term using the `intercept_` attribute.

5. **Example**: Here's an example provided in the documentation:

   ```python
   from sklearn import linear_model
   reg = linear_model.Ridge(alpha=.5)
   reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
   print(reg.coef_)  # Output: array([0.34545455, 0.34545455])
   print(reg.intercept_)  # Output: 0.13636...
   ```
   This example demonstrates fitting a Ridge regression model with three data points `[[0, 0], [0, 0], [1, 1]]` and corresponding target values `[0, .1, 1]`. After fitting, it prints out the coefficients of the linear model and the intercept term.

6. **Solver Selection**: The Ridge class allows users to specify the solver algorithm through the `solver` parameter. If set to "auto", scikit-learn will automatically choose between different solvers based on certain conditions, as described in the table provided in the documentation.

In summary, this section provides an overview of Ridge regression, its mathematical formulation, usage in scikit-learn, an example, and details on solver selection.

