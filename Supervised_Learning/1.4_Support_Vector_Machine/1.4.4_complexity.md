# 1.4.4. Complexity

1. **Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors**: This line highlights the trade-off between the power of SVMs as a machine learning tool and their computational and memory demands. As the number of training vectors increases, the computational and storage requirements of SVMs also increase rapidly.

2. **The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data**: This line explains that the fundamental task of an SVM involves solving a quadratic programming problem. The goal is to find a decision boundary that maximally separates support vectors (data points closest to the decision boundary) from the rest of the training data.

3. **The QP solver used by the libsvm-based implementation scales between 
 and 
 depending on how efficiently the libsvm cache is used in practice (dataset dependent)**: This line discusses the computational complexity of solving the quadratic programming problem, which depends on the specific implementation and how efficiently it utilizes caching mechanisms. The complexity is typically between quadratic and cubic in terms of the number of training vectors.

4. **If the data is very sparse 
 should be replaced by the average number of non-zero features in a sample vector**: Here, it's mentioned that in the case of very sparse data (where most feature values are zero), the complexity expression should be adjusted to consider the average number of non-zero features in a sample vector instead of the total number of features.

5. **For the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features**: This line highlights the efficiency of the LinearSVC algorithm, particularly for large datasets with millions of samples and/or features. Unlike the libsvm-based SVC, LinearSVC can scale almost linearly in terms of computational complexity with the size of the dataset.

In summary, this documentation provides insights into the computational complexity of SVMs, particularly emphasizing the quadratic programming problem at the core of SVMs and the efficiency differences between libsvm-based and liblinear-based implementations for linear SVMs.


<br> <br> 

### Quadratic Programming Problem:
`Quadratic Programming (QP) is a mathematical optimization problem that involves minimizing or maximizing a quadratic objective function subject to linear equality and inequality constraints. In the context of Support Vector Machines (SVMs), QP plays a central role in finding the optimal hyperplane that separates different classes of data with the largest possible margin.`
