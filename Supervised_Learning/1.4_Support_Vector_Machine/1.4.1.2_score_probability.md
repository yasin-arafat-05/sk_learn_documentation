
# 1.4.1.2. Scores and probabilities:

1. **"The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case)."**:

```python
from sklearn import svm

# Sample data
X = [[0, 0], [1, 1], [2, 2]]
y = [0, 1, 0]

# Initialize SVC classifier
clf = svm.SVC()

# Fit the classifier
clf.fit(X, y)

# Decision scores for each sample
decision_scores = clf.decision_function(X)
print("Decision scores:", decision_scores)
```
Certainly! Let's break down each line of code:

1. **Importing the Support Vector Machine (SVM) module**:
   ```python
   from sklearn import svm
   ```
   - This line imports the SVM module from the scikit-learn library, which contains classes and functions related to Support Vector Machines.

2. **Sample Data Definition**:
   ```python
   # Sample data
   X = [[0, 0], [1, 1], [2, 2]]
   y = [0, 1, 0]
   ```
   - Here, we define our sample data `X`, which is a list of feature vectors. Each feature vector represents a sample, where the first element corresponds to the first feature and the second element corresponds to the second feature.
   - `y` is the corresponding list of labels, where each label indicates the class to which the corresponding sample belongs.

3. **Initializing the Support Vector Classification (SVC) Classifier**:
   ```python
   # Initialize SVC classifier
   clf = svm.SVC()
   ```
   - We initialize an instance of the Support Vector Classification (SVC) classifier using `svm.SVC()`. This creates a classifier object with default parameters.

4. **Fitting the Classifier**:
   ```python
   # Fit the classifier
   clf.fit(X, y)
   ```
   - The `fit` method is called on the classifier object (`clf`) to train the classifier on the provided sample data (`X`) and corresponding labels (`y`). This step involves determining the decision boundary that best separates the different classes in the feature space.

5. **Obtaining Decision Scores**:
   ```python
   # Decision scores for each sample
   decision_scores = clf.decision_function(X)
   ```
   - The `decision_function` method is used to obtain decision scores for each sample in the dataset (`X`). Decision scores represent the signed distance of each sample to the decision boundary. Positive scores indicate one class, while negative scores indicate the other class.
   - The resulting `decision_scores` variable contains an array of decision scores corresponding to each sample.

6. **Printing Decision Scores**:
   ```python
   print("Decision scores:", decision_scores)
   ```
   - Finally, the decision scores are printed to the console for inspection. This allows us to observe the model's confidence in classifying each sample based on the obtained decision scores.

This code demonstrates the process of training an SVC classifier on sample data and obtaining decision scores for each sample, providing insights into the classifier's behavior and confidence in its predictions.

আমরা যে  প্রতেকটা class এর জন্য একটা number set করেছিলাম one-vs-all এ decision_score হচ্ছে সেই জিনিস । সেইটার মতো এখানে যার পজিটিভ ভ্যালু বেশি থাকবে আমরা তাকে piority দিবো । আর decision boundry হচ্ছে hyperplane . 

<br> <br> 

2. **"When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled."**:

```python
# Initialize SVC classifier with probability=True
clf = svm.SVC(probability=True)

# Fit the classifier
clf.fit(X, y)

# Predict class probabilities for each sample
class_probabilities = clf.predict_proba(X)
print("Class probabilities:", class_probabilities)
```

Certainly! Let's break down each line of code:

1. **Initializing the SVC Classifier with Probability Estimation**:
   ```python
   # Initialize SVC classifier with probability=True
   clf = svm.SVC(probability=True)
   ```
   - Here, we're initializing an instance of the Support Vector Classification (SVC) classifier from scikit-learn's SVM module.
   - We set the `probability` parameter to `True`. This parameter controls whether to enable probability estimates. When set to `True`, it allows the classifier to estimate class probabilities using **Platt scaling**.

---

3. **"In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM’s scores, fit by an additional cross-validation on the training data."**:

```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrate probabilities using Platt scaling
calibrated_clf = CalibratedClassifierCV(base_estimator=clf, method='sigmoid', cv='prefit')
calibrated_clf.fit(X, y)

# Predict calibrated probabilities
calibrated_probabilities = calibrated_clf.predict_proba(X)
print("Calibrated probabilities:", calibrated_probabilities)
```

Let's break down each line of code and the concepts involved:

1. **Importing the `CalibratedClassifierCV`**:
   ```python
   from sklearn.calibration import CalibratedClassifierCV
   ```
   - This line imports the `CalibratedClassifierCV` class from scikit-learn's calibration module. `CalibratedClassifierCV` is a meta-estimator that calibrates the output probabilities of a classifier.

2. **Initializing the Calibrated Classifier**:
   ```python
   calibrated_clf = CalibratedClassifierCV(base_estimator=clf, method='sigmoid', cv='prefit')
   ```
   - Here, we initialize an instance of `CalibratedClassifierCV`.
   - `base_estimator=clf` specifies the base classifier (in this case, `clf`, which is the previously trained SVM classifier) whose probabilities will be calibrated.
   - `method='sigmoid'` indicates the calibration method. `'sigmoid'` refers to Platt scaling, which fits a logistic regression model to the output scores of the base classifier.
   - `cv='prefit'` specifies the cross-validation strategy. Since we're providing pre-fitted data, we set `cv='prefit'`.

3. **Fitting the Calibrated Classifier**:
   ```python
   calibrated_clf.fit(X, y)
   ```
   - The `fit` method is called on the calibrated classifier (`calibrated_clf`) to train the calibration model.
   - During training, Platt scaling fits a logistic regression model to the decision scores produced by the base classifier (`clf`) using the provided training data (`X`) and labels (`y`).

4. **Predicting Calibrated Probabilities**:
   ```python
   calibrated_probabilities = calibrated_clf.predict_proba(X)
   ```
   - After fitting the calibrated classifier, the `predict_proba` method is called to predict calibrated probabilities for each sample in the dataset `X`.
   - The resulting `calibrated_probabilities` variable contains the predicted probabilities for each class for each sample, after calibration.

5. **Printing Calibrated Probabilities**:
   ```python
   print("Calibrated probabilities:", calibrated_probabilities)
   ```
   - Finally, the calibrated probabilities are printed to the console for inspection.

In summary:
- **Calibration**: The process of adjusting the output probabilities of a classifier to improve their accuracy and reliability.
- **Cross-Validation**: A technique used to evaluate the performance of a model by splitting the data into multiple subsets (folds) for training and validation.
- **Platt Scaling**: A specific calibration method that involves fitting a logistic regression model to the output scores of the classifier to obtain calibrated probabilities.

---

<br> <br>


4. **"The cross-validation involved in Platt scaling is an expensive operation for large datasets."**:
   - No code example provided as this is a statement about the computational cost of the calibration process.

5. **"In addition, the probability estimates may be inconsistent with the scores:"**:
   - No specific code example, as this is a statement about potential inconsistencies between probability estimates and decision scores.

6. **"Platt’s method is also known to have theoretical issues."**:
   - No specific code example, as this is a statement about theoretical limitations of Platt scaling.

7. **"If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba."**:

```python
# Initialize SVC classifier with probability=False
clf = svm.SVC(probability=False)

# Fit the classifier
clf.fit(X, y)

# Decision scores for each sample
decision_scores = clf.decision_function(X)
print("Decision scores (confidence scores):", decision_scores)
```

8. **"Please note that when decision_function_shape='ovr' and n_classes > 2, unlike decision_function, the predict method does not try to break ties by default."**:
   - No specific code example provided, but this statement highlights behavior of predict method in certain settings.

9. **"See SVM Tie Breaking Example for an example on tie breaking."**:
   - Refers to a provided example in the documentation for understanding tie-breaking in SVM predictions.