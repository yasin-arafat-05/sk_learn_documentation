
# 1.4.1.2. Scores and probabilities:

1. **"The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case)."**:

```python
from sklearn import svm

# Sample data
X = [[0, 0], [1, 1], [2, 2]]
y = [0, 1, 0]

# Initialize SVC classifier
clf = svm.SVC()

# Fit the classifier
clf.fit(X, y)

# Decision scores for each sample
decision_scores = clf.decision_function(X)
print("Decision scores:", decision_scores)
```
Certainly! Let's break down each line of code:

1. **Importing the Support Vector Machine (SVM) module**:
   ```python
   from sklearn import svm
   ```
   - This line imports the SVM module from the scikit-learn library, which contains classes and functions related to Support Vector Machines.

2. **Sample Data Definition**:
   ```python
   # Sample data
   X = [[0, 0], [1, 1], [2, 2]]
   y = [0, 1, 0]
   ```
   - Here, we define our sample data `X`, which is a list of feature vectors. Each feature vector represents a sample, where the first element corresponds to the first feature and the second element corresponds to the second feature.
   - `y` is the corresponding list of labels, where each label indicates the class to which the corresponding sample belongs.

3. **Initializing the Support Vector Classification (SVC) Classifier**:
   ```python
   # Initialize SVC classifier
   clf = svm.SVC()
   ```
   - We initialize an instance of the Support Vector Classification (SVC) classifier using `svm.SVC()`. This creates a classifier object with default parameters.

4. **Fitting the Classifier**:
   ```python
   # Fit the classifier
   clf.fit(X, y)
   ```
   - The `fit` method is called on the classifier object (`clf`) to train the classifier on the provided sample data (`X`) and corresponding labels (`y`). This step involves determining the decision boundary that best separates the different classes in the feature space.

5. **Obtaining Decision Scores**:
   ```python
   # Decision scores for each sample
   decision_scores = clf.decision_function(X)
   ```
   - The `decision_function` method is used to obtain decision scores for each sample in the dataset (`X`). Decision scores represent the signed distance of each sample to the decision boundary. Positive scores indicate one class, while negative scores indicate the other class.
   - The resulting `decision_scores` variable contains an array of decision scores corresponding to each sample.

6. **Printing Decision Scores**:
   ```python
   print("Decision scores:", decision_scores)
   ```
   - Finally, the decision scores are printed to the console for inspection. This allows us to observe the model's confidence in classifying each sample based on the obtained decision scores.

This code demonstrates the process of training an SVC classifier on sample data and obtaining decision scores for each sample, providing insights into the classifier's behavior and confidence in its predictions.

<br> <br> 

2. **"When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled."**:

```python
# Initialize SVC classifier with probability=True
clf = svm.SVC(probability=True)

# Fit the classifier
clf.fit(X, y)

# Predict class probabilities for each sample
class_probabilities = clf.predict_proba(X)
print("Class probabilities:", class_probabilities)
```

3. **"In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM’s scores, fit by an additional cross-validation on the training data."**:

```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrate probabilities using Platt scaling
calibrated_clf = CalibratedClassifierCV(base_estimator=clf, method='sigmoid', cv='prefit')
calibrated_clf.fit(X, y)

# Predict calibrated probabilities
calibrated_probabilities = calibrated_clf.predict_proba(X)
print("Calibrated probabilities:", calibrated_probabilities)
```

4. **"The cross-validation involved in Platt scaling is an expensive operation for large datasets."**:
   - No code example provided as this is a statement about the computational cost of the calibration process.

5. **"In addition, the probability estimates may be inconsistent with the scores:"**:
   - No specific code example, as this is a statement about potential inconsistencies between probability estimates and decision scores.

6. **"Platt’s method is also known to have theoretical issues."**:
   - No specific code example, as this is a statement about theoretical limitations of Platt scaling.

7. **"If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba."**:

```python
# Initialize SVC classifier with probability=False
clf = svm.SVC(probability=False)

# Fit the classifier
clf.fit(X, y)

# Decision scores for each sample
decision_scores = clf.decision_function(X)
print("Decision scores (confidence scores):", decision_scores)
```

8. **"Please note that when decision_function_shape='ovr' and n_classes > 2, unlike decision_function, the predict method does not try to break ties by default."**:
   - No specific code example provided, but this statement highlights behavior of predict method in certain settings.

9. **"See SVM Tie Breaking Example for an example on tie breaking."**:
   - Refers to a provided example in the documentation for understanding tie-breaking in SVM predictions.