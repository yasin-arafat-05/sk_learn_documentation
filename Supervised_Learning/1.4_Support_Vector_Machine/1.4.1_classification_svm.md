
# 1.4.1. Classification In SVM:

---

**1.4.1. Classification**

**SVC, NuSVC, and LinearSVC**

- **Description**: 
    - `SVC`, `NuSVC`, and `LinearSVC` are classes capable of performing binary and multi-class classification on a dataset.
- **Differences**:
    - `SVC` and `NuSVC` are similar methods but accept slightly different sets of parameters and have different mathematical formulations.
    - `LinearSVC` is another (faster) implementation of Support Vector Classification for the case of a linear kernel. It lacks some attributes of `SVC` and `NuSVC`, like `support_`.
- **Mathematical Formulation**:
    - Detailed mathematical formulations are mentioned in the respective sections.
- **Input**:
    - Two arrays: `X` of shape `(n_samples, n_features)` holding the training samples, and `y` of class labels (strings or integers), of shape `(n_samples)`.
- **Example Usage**:
    ```python
    from sklearn import svm
    X = [[0, 0], [1, 1]]
    y = [0, 1]
    clf = svm.SVC()
    clf.fit(X, y)
    print(clf.predict([[2., 2.]]))
    ```
- **Support Vectors**:
    - `SVMs` decision function depends on a subset of the training data called the support vectors.
    - Some properties of these support vectors can be found in attributes `support_vectors_`, `support_`, and `n_support_`.
```python
from sklearn import svm

# Sample data
X = [[0, 0], [1, 1]]
y = [0, 1]

# Initialize and train SVM classifier
clf = svm.SVC()
clf.fit(X, y)

# Get support vectors
print("# get support vectors")
print(clf.support_vectors_)

# Get indices of support vectors
print("# get indices of support vectors")
print(clf.support_)

# Get number of support vectors for each class
print("# get number of support vectors for each class")
print(clf.n_support_)
```

**Examples**:
- SVM: Maximum margin separating hyperplane
- Non-linear SVM
- SVM-Anova: SVM with univariate feature selection


**Squared Hinge Loss**:
- `LinearSVC` uses the squared hinge loss function. The hinge loss is a loss function used for training classifiers, particularly SVMs. It penalizes misclassifications linearly and is defined as the maximum of 0 and the difference between 1 and the product of the true label and the predicted value.
- Squared hinge loss is simply the square of the hinge loss function. This modification can provide better numerical stability and faster convergence during training compared to the original hinge loss.

**Regularization of Intercept**:
- In the `LinearSVC` implementation provided by `liblinear`, the intercept term (bias) is also regularized if considered. Regularization helps prevent overfitting by penalizing large coefficients. Including the intercept term in the regularization process helps control model complexity and generalization.
- However, the regularization effect on the intercept might not always be desirable, especially if the intercept is of less importance or if it introduces unwanted bias into the model.
- To mitigate the regularization effect on the intercept, the `intercept_scaling` parameter can be fine-tuned. This parameter allows the intercept term to have a different regularization behavior compared to the other features. By carefully adjusting `intercept_scaling`, one can control the impact of the intercept regularization on the overall model.

**Impact on Classification Results**:
- Due to the squared hinge loss and regularization of the intercept, `LinearSVC` may produce different classification results and scores compared to `SVC` and `NuSVC`.
- The regularization of the intercept and its tuning through `intercept_scaling` can affect the bias-variance tradeoff of the model and influence its generalization performance.
- It's essential to experiment with different settings of `intercept_scaling` to find the optimal configuration for a given dataset and classification task.

In summary, `LinearSVC` offers efficient training for linear SVMs using the squared hinge loss and provides options to control the regularization behavior of the intercept through the `intercept_scaling` parameter, which can impact the classification results and model performance.

---

If you need a practical code example, here's how you can train an SVM classifier using `SVC` on a dataset:

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train SVM classifier
clf = SVC()
clf.fit(X_train_scaled, y_train)

# Predictions
y_pred = clf.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

This code snippet demonstrates how to:

- Load the Iris dataset.
- Split the dataset into training and testing sets.
- Scale the features.
- Initialize and train an SVM classifier (`SVC`).
- Make predictions.
- Calculate the accuracy of the model.

You can replace `SVC()` with `NuSVC()` or `LinearSVC()` for different implementations. Adjust parameters as necessary.