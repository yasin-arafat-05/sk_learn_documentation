
# 1.4.1. Classification In SVM:

---

**1.4.1. Classification**

**SVC, NuSVC, and LinearSVC**

- **Description**: 
    - `SVC`, `NuSVC`, and `LinearSVC` are classes capable of performing binary and multi-class classification on a dataset.
- **Differences**:
    - `SVC` and `NuSVC` are similar methods but accept slightly different sets of parameters and have different mathematical formulations.
    - `LinearSVC` is another (faster) implementation of Support Vector Classification for the case of a linear kernel. It lacks some attributes of `SVC` and `NuSVC`, like `support_`.
- **Mathematical Formulation**:
    - Detailed mathematical formulations are mentioned in the respective sections.
- **Input**:
    - Two arrays: `X` of shape `(n_samples, n_features)` holding the training samples, and `y` of class labels (strings or integers), of shape `(n_samples)`.
- **Example Usage**:
    ```python
    from sklearn import svm
    X = [[0, 0], [1, 1]]
    y = [0, 1]
    clf = svm.SVC()
    clf.fit(X, y)
    print(clf.predict([[2., 2.]]))
    ```
- **Support Vectors**:
    - `SVMs` decision function depends on a subset of the training data called the support vectors.
    - Some properties of these support vectors can be found in attributes `support_vectors_`, `support_`, and `n_support_`.
```python
from sklearn import svm

# Sample data
X = [[0, 0], [1, 1]]
y = [0, 1]

# Initialize and train SVM classifier
clf = svm.SVC()
clf.fit(X, y)

# Get support vectors
print("# get support vectors")
print(clf.support_vectors_)

# Get indices of support vectors
print("# get indices of support vectors")
print(clf.support_)

# Get number of support vectors for each class
print("# get number of support vectors for each class")
print(clf.n_support_)
```

**Examples**:
- SVM: Maximum margin separating hyperplane
- Non-linear SVM
- SVM-Anova: SVM with univariate feature selection

---

If you need a practical code example, here's how you can train an SVM classifier using `SVC` on a dataset:

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train SVM classifier
clf = SVC()
clf.fit(X_train_scaled, y_train)

# Predictions
y_pred = clf.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```

This code snippet demonstrates how to:

- Load the Iris dataset.
- Split the dataset into training and testing sets.
- Scale the features.
- Initialize and train an SVM classifier (`SVC`).
- Make predictions.
- Calculate the accuracy of the model.

You can replace `SVC()` with `NuSVC()` or `LinearSVC()` for different implementations. Adjust parameters as necessary.