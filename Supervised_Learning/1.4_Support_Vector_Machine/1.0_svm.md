
# 1.4. Support Vector Machines:

### Support Vector Machine (SVM) Part-
[Video No(32)](https://www.youtube.com/watch?v=xLkk6MUrvrw&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=33&ab_channel=5MinutesEngineering)

In supervised learning classification problem 
আমরা label data দেয় model কে train করার জন্য । এখানে, চর্তভূজ আর  বৃত্ত দিয়েছি । 

![Alt text](/Supervised_Learning/1.4_Support_Vector_Machine/image/image.png)

এখন আমরা circle class and quadrilateral class কে আলাদা করে এমন একটা লাইন বানাবো । এই লাইন কে decision boundary বা hyperplane বলে । 

আমরা circle class এর এমন একটা Data point বের করবো যেইটা opponent class বা quadrilateral class এর খুব কাছাকাছি । তারপর সেই ডাটা পয়েন্ট এর কে প্রায়  স্পর্শ করে যায় এমন একটা লাইন টানবো ।  Quadrilateral class এর জন্য ও সেম করবো । 

এই দুইটা parallel লাইনের মধ্যবর্তী দুরত্বকে margin বলে ।  আর আমরা যেই দুইটা data point দিয়ে parallel লাইন দুটো এঁকেছিলাম সেই দুইটা data point  কে support vector বলে । (** প্রশ্ন এখন,  parallel লাইন তো অনেক গুলো আকা যাইতো তাইলে কেন শুধু আমরা ছবির মতোয় নিলাম ?? ) **


### Support Vector Machine (SVM) Part-2
[Video No(33)](https://www.youtube.com/watch?v=0MJTaPoHv-g&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=33&ab_channel=5MinutesEngineering)

SVM এর part one এ আমরা linear separate data নিয়ে কাজ  করেছি  । Linear separate Data সেই ডাটা যাদেরকে এক বা একাধিক লাইন দিয়ে আমরা আলাদা করতে পারি ।

![Alt text](/Supervised_Learning/1.4_Support_Vector_Machine/image/image-1.png)

আমরা এমন লাইকে hyperplane ধরবো যার margin সবচেয়ে বেশি হয় । তাই আমরা 2nd গ্রাফ accurate বলে গণ্য করি। 

Margin বেশি রাখলে সেইটা future prediction এর জন্য ভালো হয় । পরবর্তীতে new data আসলে misclassifiction কম হবে । অর্থাৎ, model এর prediction generation করতে তেমন কোন  ঝামেলা হবে না , model এর error rate কম থাকবে । 


### Support Vector Machine (SVM) Part-3
[Video No(34)](https://www.youtube.com/watch?v=owsAQ_fiwIw&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=34&ab_channel=5MinutesEngineering)

![Alt text](/Supervised_Learning/1.4_Support_Vector_Machine/image/image-2.png)

Graph A -> Linear Data.
Graph B -> Nonlinear Data.
এখানে Graph(B) এ  আমরা  একটা line টেনে দুইটা class কে আলাদা করতে পারবো না । 

![Alt text](/Supervised_Learning/1.4_Support_Vector_Machine/image/image-3.png)

Kernel function:
 LD (Low Dimension ) feature কে  HD(High Dimension)  featue এ convert করে । Input  এ আমাদের কাছে 2D feature ছিল kernel function apply করার পর সেইটা কে 3d তে convert করেছি । 

Convert করার পর আমরা একটা 2D plane দিয়ে class গুলোকে আলাদা করি । 

<br> <br> 

Support Vector Machines (SVMs) are powerful tools for classification, regression, and outlier detection. They come with several advantages:

1. **Effectiveness in High Dimensional Spaces**: SVMs perform well even when dealing with data that has a high number of dimensions.
**High Dimensinal Spaces:** <br>
`High-dimensional spaces refer to spaces where the data has a large number of features or dimensions. In simpler terms, it means each data point is described by a large number of variables. For example, consider a dataset where each data point represents a house, and the features include the number of rooms, the square footage, the neighborhood crime rate, the distance to the nearest school, etc. If you have many such features, you're dealing with a high-dimensional space.`

2. **Effectiveness with Small Sample Size**: They remain effective even when the number of dimensions is greater than the number of samples.
**Number of samples:** <br>
`The "number of samples" typically refers to the number of data points or instances in a dataset.`


3. **Memory Efficiency**: SVMs use a subset of training points (support vectors) in the decision function, making them memory efficient, especially for large datasets.

4. **Versatility**: SVMs allow for the use of different kernel functions for the decision function, making them adaptable to various types of data distributions. Common kernels are provided, and custom kernels can also be specified.

However, there are also some disadvantages to using SVMs:

1. **Risk of Overfitting**: When the number of features is significantly larger than the number of samples, choosing appropriate kernel functions and regularization terms becomes crucial to avoid overfitting.

2. **Lack of Direct Probability Estimates**: SVMs do not directly provide probability estimates for predictions. Calculating these probabilities typically involves using expensive techniques like five-fold cross-validation.

Here's a real-life code example of using SVMs for classification using scikit-learn in Python:

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the iris dataset (a standard machine learning dataset)
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize SVM classifier with a linear kernel
svm_classifier = SVC(kernel='linear')

# Fit the classifier to the training data
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
```
This example demonstrates how to use SVMs for classification using the Iris dataset. The `SVC` class from scikit-learn is used to create an SVM classifier with a linear kernel. The classifier is then trained on the training data and used to make predictions on the test data. Finally, the accuracy of the predictions is calculated and printed.



<br> <br> 

### **Explanation:**

#### X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

```python
# Importing necessary library
from sklearn.model_selection import train_test_split

# Assuming we have a dataset X with features and y with target labels
# Example dataset
X = [
    [100, 'red'],
    [120, 'orange'],
    [90, 'red'],
    [110, 'orange'],
    [105, 'red']
]

y = [0, 1, 0, 1, 0]  # 0 represents apple, 1 represents orange

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

Explanation of the code:

- `train_test_split`: This function is from the `sklearn.model_selection` module, used to split arrays or matrices into random train and test subsets.
- `X`: This variable represents the features of the dataset, such as weight and color of fruits.
- `y`: This variable represents the target labels, indicating whether the fruit is an apple or an orange.
- `test_size`: This parameter determines the size of the testing set relative to the whole dataset. Here, `test_size=0.3` means that 30% of the data will be reserved for testing, and the remaining 70% will be used for training.
- `random_state`: This parameter sets the random seed for reproducibility. It ensures that the data splitting is consistent across runs.

After splitting the data, we have:

- `X_train`: This variable contains the features of the training set.
- `X_test`: This variable contains the features of the testing set.
- `y_train`: This variable contains the target labels corresponding to the training set.
- `y_test`: This variable contains the target labels corresponding to the testing set.

Now, let's represent these splits in tabular form:

| X_train | weight | color   |
|---------|--------|---------|
| 1       | 120    | 'orange'|
| 2       | 100    | 'red'   |
| 3       | 105    | 'red'   |

| X_test | weight | color   |
|--------|--------|---------|
| 0      | 90     | 'red'   |
| 4      | 105    | 'red'   |

| y_train |
|---------|
| 1       |
| 0       |
| 0       |

| y_test |
|--------|
| 0      |
| 1      |

In these tables:

- `X_train` consists of 70% of the original dataset used for training the model.
- `X_test` consists of 30% of the original dataset used for evaluating the model's performance.
- `y_train` contains the corresponding labels for the training set.
- `y_test` contains the corresponding labels for the testing set.

These splits allow us to train our model on the training data and then evaluate its performance on the unseen testing data to assess its generalization ability.

